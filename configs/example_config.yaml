# Config example file to configure a data pipeline for the Standalone Environment.
#
# "interval", number of seconds between executions. Optional, default value 60.
# "exporter", string, where to export data. Possible values: "nrinfra", "nrmetrics", "nrevents", "nrlogs", "nrtraces", "otel" and "prom"
# "buffer", size of channel buffers. Optional, default value 100.

interval: 60

# Custom keys here...

# Size of communication buffer between modules
# buffer: 500

# New Relic API credentials
nr_account_id: xxxxx
nr_user_key: xxxxx
nr_api_key: xxxxx
nr_endpoint: US

# Databricks Credentials
# https://docs.databricks.com/en/dev-tools/auth/pat.html#databricks-personal-access-tokens-for-workspace-users
db_access_token: xxxxx

# Spark Driver Proxy API Endpoint
# Ref: https://databrickslabs.github.io/overwatch/assets/_index/realtime_helpers.html
# Run a Scala notebook to get this, use the resultant `url`
# val env = dbutils.notebook.getContext.apiUrl.get
# val token = dbutils.notebook.getContext.apiToken.get
# val clusterId = spark.conf.get("spark.databricks.clusterUsageTags.clusterId")
# val sparkContextID = spark.conf.get("spark.databricks.sparkContextId")
# val orgId = spark.conf.get("spark.databricks.clusterUsageTags.clusterOwnerOrgId")
# val uiPort = spark.conf.get("spark.ui.port")
# val apiPath = "applications"
# val url = s"$env/driver-proxy-api/o/$orgId/$clusterId/$uiPort"
spark_endpoint: xxxxx

# Your databricks workspace URL
# https://docs.databricks.com/en/dev-tools/auth/index.html#databricks-account-and-workspace-rest-apis
databricks_endpoint: xxxxx #

# Azure | AWS | GCP
db_cloud: xxx

# Exporter batching and timing
batch_size: 50
harvest_time: 10
